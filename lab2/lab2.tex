% Lab 2: Machine Learning in Motion
% Author: Rob Kelly
%
% CSE/IT 489/589-06: Introduction to Neural Network Applications
% Spring 2016
% New Mexico Tech
%
% Lab Goal: (spooky skeleton)
% \_ Using WEKA and interpreting results
% |  \_ Open a dataset (data/segment-challenge.arff)
% |  \_ Train LVQ1 like in lab1
% |  \_ Question: Check out the confusion matrix. Which class was most often 
% |      mis-classified? Which class were other instances most-often
% |      mis-classified as? Conjecture about what each of those facts might
% |      imply.
% \_ Tuning parameters to what you know
%    \_ Open soybean.arff
%    \_ Default parameterization has 55.7% RAE. Surely we can do better.
%    \_ Raise totalCodebookVectors to 100 and train LVQ1 again; expect 20.3% RAE
%    |  \_ Question: Examine the results report for the model with 20 codebook
%    |      vectors and the report for the model with 100 -- in particular, look
%    |      at the class distribution section. What was the problem with the
%    |      first model, and why did changing the number of codebook vectors
%    |      help?
%    \_ Raise totalTrainingIterations to 2000; expect 16.5% RAE.
%       \_ Question: Let's see if training this model for longer simply works
%           better. Train LVQ1s with the same parameters for 100, 1000, 10000,
%           and 100000 iterations. (the last one might take a little while.)
%           Estimate where there might be a "sweet spot" of training time.
%           Why doesn't training longer simply make the LVQ model better?
%

\documentclass[11pt]{cselabheader}

\usepackage{enumitem}

\fancyhead[R]{Lab 2: Machine Learning in Motion}
\title{Neural Network Applications -- Lab 2 \\ Machine Learning in Motion}

\begin{document}
\maketitle

\horrule{0.5pt}\\\horrule{2pt}

\section{Background}

In the last lab activity, you started getting familiar with \textbf{WEKA}, the data mining toolkit which we'll use frequently in this lab course. Now it's time to start using it for real. Today, you'll be applying a little bit of the theory you've learned in lecture to classification tasks using the LVQ neural network algorithm.

LVQ is a \textit{prototype-based}, \textit{winner-take-all} supervised classification algorithm. It begins with some number of prototype vectors (sometimes called \textit{codebook vectors}) in feature-space, each with some class. For each vector in the training set, it selects the codebook vector that is closest to the the input. If the classification is correct, the codebook vector is moved closer to the input at some learning rate, or further away if incorrect.

Our distribution of WEKA contains a few different implementations of the same basic algorithm\footnote{You can learn more here: \url{http://www.cis.hut.fi/research/lvq_pak/lvq_doc.txt}}, each with different strengths for different applications. In the field, LVQ variants have been particularly useful in classification of text documents\footnote{Fahad and Sikander (2007). ``Classification of textual documents using learning vector quantization.'' Information Technology Journal 6 (1): 154â€“159.} and optical character recognition\footnote{Loncelle, Derycke, Fogelman (1992). ``Cooperation of GBP and LVQ networks for optical character recognition.'' International Joint Conference on Neural Networks 1992 (3): 694-699.}, among other tasks. In this lab, we'll investigate how to interpret the results of WEKA's LVQ1 classifier, and we'll apply the theory we've learned to ``tune'' the algorithm to a particular dataset.

\pagebreak

\section{Lab Exercises}

\subsection{Understanding WEKA output}

\begin{itemize}[leftmargin=*]
  \item Open \texttt{data/segment-challenge.arff} in the WEKA explorer. You might remember this from the last lab, when you trained an LVQ network to classify instances from this image-segment dataset.

  \item Train an \texttt{Lvq1} to classify the dataset. Use 66\% of the dataset to train, and the remainder to test.

  \begin{ex}[Confusing Results]
    Which class was most often mis-classified by the model? As which class were other instances most often mis-classified? Briefly, conjecture about what these results might mean.
  \end{ex}
\end{itemize}

\subsection{Tuning your Hyperparameters}

\begin{itemize}[leftmargin=*]
  \item Open \texttt{data/soybean.arff} in the WEKA explorer. This is a large dataset with a wide variety of classes; we'll be using it to make a point about classifiers.

  \begin{infobox}{ASTOUNDING DATA FACTZ}
    Nobody asked, but... This thing is called the \textit{Large Soybean Database}. Wow. Each instance is a vector of features of a soybean plant, each classified by the crop disease afflicting it.

    Soybeans are important.
  \end{infobox}

  \item Train an \texttt{Lvq1} classifier on this dataset, using the default parameterization just as you did in the last section: 20 codebook vectors over 1000 training epochs. Test it on a 66\% split.
  \begin{itemize}[leftmargin=*]
    \item \textit{Your testing should show a Relative Absolute Error (RAE) of about 55\%. (That's not very good.)}
  \end{itemize}
  
  \item Increase the \texttt{totalCodebookVectors} hyperparameter from \textbf{20} (default) to \textbf{100}. This is the number of codebook -- or \textit{prototype} -- vectors the LVQ algorithm will train.

  \begin{ex}[A Few Codebook Vectors More]
    Examine the results report for the model with 20 codebook vectors and the report for the one with 100 -- in particular, look at the class distribution section. What was the problem with the first model? Why did changing the number of codebook vectors help fix the problem?
  \end{ex}

  \begin{infobox}{WOAH LOOK OUT}
    In the \texttt{Lvq1} classifier output in WEKA, the distribution of codebook vectors across the dataset's classes is reported under the ``Classifier model (full training set)'' heading. Specifically, it's in the ``Cass Distribution'' section. Yes, that's supposed to be \textit{Class}.
  \end{infobox}

\pagebreak

  \item Increase the \texttt{totalTrainingIterations} hyperparameter from \textbf{1000} to \textbf{2000}.
  \begin{itemize}[leftmargin=*]
    \item For the LVQ algorithm, we know that a network with more codebook vectors needs more training, so we can reasonably expect bumping up the training time will help.
  \end{itemize}

  \begin{ex}[Rules of Thumb]
    Let's see if training this model for longer simply works better in general. Train \texttt{Lvq1} models with the same parameters for 100, 1000, 10000, and 100000 iterations. (That last one might take a few seconds.)

    Estimate where there might be a ``sweet spot'' of training time, where RAE is \textit{generally} minimized for this model on this data. It might help to play around with different levels of training time in between what you've just tried. Why doesn't training this LVQ network for \textit{longer} necessarily make it \textit{better}?
  \end{ex}
\end{itemize}

\pagebreak

\section{Submitting}

Collect your solutions to Exercises 2.1, 2.2, and 2.3 in one document. PDF or plaintext format are both fine. This lab is to be submitted as a group; name your solutions document \texttt{group\_\#\_lab2.tar.gz} (where \# denotes your group's number), and submit it on the \textit{Lab 2} assignment page on the course Canvas page.

\end{document}